{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Display metadata\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "non_partitioned_table_path = '/home/iceberg/warehouse/transformed/iceberg_demo'\n",
    "partitioned_table_path = '/home/iceberg/warehouse/transformed/iceberg_partitioned_demo'\n",
    "\n",
    "\n",
    "catalog_name = \"iceberg_catalog\"\n",
    "database_name = \"transformed\"\n",
    "table_name = \"iceberg_demo\"\n",
    "table_name_partitioned = \"iceberg_partitioned_demo\"\n",
    "\n",
    "def display_metadata(demo_tbl_df, table_path):\n",
    "\n",
    "    \n",
    "    metadata_files_path = os.path.join(table_path, 'metadata/*.json')\n",
    "    meta_data_files = sorted(glob.iglob(metadata_files_path), key=os.path.getctime, reverse=True)\n",
    "\n",
    "    data_files_path = os.path.join(table_path, 'data/*')\n",
    "    data_files = sorted(glob.iglob(data_files_path), key=os.path.getctime, reverse=True)\n",
    "\n",
    "    print(\"---------------- Data Files List -----------------------\")\n",
    "    data_files_df = spark.createDataFrame(data_files, StringType(), )\n",
    "    display(data_files_df.toPandas())\n",
    "\n",
    "    print(\"---------------- Meta Data Files List -----------------------\")\n",
    "    metadata_files_df = spark.createDataFrame(meta_data_files, StringType())\n",
    "    display(metadata_files_df.toPandas())\n",
    "\n",
    "    print(\"---------------- Meta Data Json -----------------------\")\n",
    "    latest_metadata_file = meta_data_files[0]\n",
    "    metadata_df = spark.read.format(\"json\").option(\"multiLine\", True).load(latest_metadata_file).select(\n",
    "        F.col(\"current-schema-id\"),\n",
    "        F.col(\"current-snapshot-id\"),\n",
    "        F.col(\"current-schema-id\"),\n",
    "        F.col(\"partition-spec\"),\n",
    "        F.col(\"schemas\"),\n",
    "        F.explode(F.col(\"snapshots\")).alias(\"snapshot\")\n",
    "    ).select(\n",
    "        F.col(\"current-schema-id\"),\n",
    "        F.col(\"current-snapshot-id\"),\n",
    "        F.col(\"current-schema-id\"),\n",
    "        F.col(\"partition-spec\"),\n",
    "        F.col(\"snapshot.snapshot-id\"),\n",
    "        F.col(\"snapshot.manifest-list\"),\n",
    "        F.explode(F.col(\"schemas\")).alias(\"schema\")\n",
    "    ).select(\n",
    "        F.col(\"current-snapshot-id\"),\n",
    "        F.col(\"snapshot-id\"),\n",
    "        F.col(\"partition-spec\"),\n",
    "        F.col(\"current-schema-id\"),\n",
    "        F.col(\"schema.schema-id\"),\n",
    "        F.col(\"schema.fields\").alias(\"schema\"),\n",
    "        F.col(\"manifest-list\")\n",
    "    ).filter(F.col(\"current-schema-id\") == F.col(\"schema-id\")).filter(F.col(\"current-snapshot-id\") == F.col(\"snapshot-id\"))\n",
    "    #metadata_df.printSchema()\n",
    "    display(metadata_df.toPandas())\n",
    "\n",
    "    print(\"---------------- Manifest List Avro -----------------------\")\n",
    "    manifest_list_files = list(metadata_df.select(F.col(\"manifest-list\")).toPandas()['manifest-list'])\n",
    "    print(manifest_list_files)\n",
    "    manifest_list_df = spark.read.format(\"avro\").load(manifest_list_files).select(\n",
    "        F.col(\"added_snapshot_id\"),\n",
    "        F.col(\"added_data_files_count\"),\n",
    "        F.col(\"existing_data_files_count\"),\n",
    "        F.col(\"deleted_data_files_count\"),\n",
    "        F.col(\"added_rows_count\"),\n",
    "        F.col(\"deleted_rows_count\"),\n",
    "        F.col(\"manifest_path\")\n",
    "    )\n",
    "    display(manifest_list_df.toPandas())\n",
    "\n",
    "    print(\"---------------- Manifest Avro -----------------------\")\n",
    "    manifest_files = list(manifest_list_df.select(F.col(\"manifest_path\")).toPandas()['manifest_path'])\n",
    "    print(manifest_list_files)\n",
    "    manifest_df = spark.read.format(\"avro\").load(manifest_files).select(\n",
    "        F.col(\"*\")\n",
    "    )\n",
    "    display(manifest_df.toPandas())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iceberg_catalog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           catalog\n",
       "0  iceberg_catalog"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     namespace\n",
       "0  transformed"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\"show catalogs\").toPandas())\n",
    "spark.sql(\"CREATE DATABASE transformed\")\n",
    "display(spark.sql(\"show databases in iceberg_catalog\").toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created.\n"
     ]
    }
   ],
   "source": [
    "# Create Demo Iceberg Table\n",
    "fact_sales_df = spark.read.format(\"parquet\").load(\"/home/datalake/transformed.fact_sales.parquet/\")\n",
    "fact_sales_df.count()\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{database_name}.{table_name}\")\n",
    "os.system(f\"rm -rf {non_partitioned_table_path}\")\n",
    "\n",
    "fact_sales_df.writeTo(f\"{catalog_name}.{database_name}.{table_name}\").create()\n",
    "print(\"Table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read Iceberg Table and Display Metadata\n",
    "demo_iceberg_df = spark.table(f\"{catalog_name}.{database_name}.{table_name}\")\n",
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=non_partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Update a record in Iceberg Table\n",
    "display(demo_iceberg_df.filter(\"order_hash_key = '733398b50242a8734489a906a12a793f' and order_item_id = '1'\").withColumn(\"file_name\", F.input_file_name()).toPandas())\n",
    "iceberg_update_output = spark.sql(f\"\"\"\n",
    "UPDATE {database_name}.{table_name}\n",
    "SET shipping_country = 'USA'\n",
    "WHERE order_hash_key = '733398b50242a8734489a906a12a793f' and order_item_id = '1'\n",
    "\"\"\")\n",
    "spark.catalog.clearCache()\n",
    "display(demo_iceberg_df.filter(\"order_hash_key = '733398b50242a8734489a906a12a793f' and order_item_id = '1'\").withColumn(\"file_name\", F.input_file_name()).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=non_partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Schema Evolution - DELETE a recond\n",
    "spark.sql(f\"\"\"DELETE FROM {catalog_name}.{database_name}.{table_name} WHERE order_hash_key = '733398b50242a8734489a906a12a793f' and order_item_id = '1'\"\"\")\n",
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=non_partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Schema Evolution - ADD a Column\n",
    "display(spark.sql(f\"DESCRIBE {catalog_name}.{database_name}.{table_name}\").toPandas())\n",
    "spark.sql(f\"\"\"ALTER TABLE {catalog_name}.{database_name}.{table_name} ADD  COLUMNS (\n",
    "                shipping_city string\n",
    "                )\"\"\")\n",
    "display(spark.sql(f\"DESCRIBE {catalog_name}.{database_name}.{table_name}\").toPandas())\n",
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=non_partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Schema Evolution - Remove a Column\n",
    "display(spark.sql(f\"DESCRIBE {catalog_name}.{database_name}.{table_name}\").toPandas())\n",
    "spark.sql(f\"\"\"ALTER TABLE {catalog_name}.{database_name}.{table_name} DROP COLUMN ctrl_load_date\"\"\")\n",
    "display(spark.sql(f\"DESCRIBE {catalog_name}.{database_name}.{table_name}\").toPandas())\n",
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=non_partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Schema Evolution - Rename a Column\n",
    "display(spark.sql(f\"DESCRIBE {catalog_name}.{database_name}.{table_name}\").toPandas())\n",
    "spark.sql(f\"\"\"ALTER TABLE {catalog_name}.{database_name}.{table_name} RENAME COLUMN subtotal TO sub_total\"\"\")\n",
    "display(spark.sql(f\"DESCRIBE {catalog_name}.{database_name}.{table_name}\").toPandas())\n",
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=non_partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()\n",
    "demo_iceberg_final = spark.table(f\"{catalog_name}.{database_name}.{table_name}\")\n",
    "display(demo_iceberg_final.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Partitioned Iceberg Table\n",
    "\n",
    "fact_sales_df = spark.read.format(\"parquet\").load(\"/home/datalake/transformed.fact_sales.parquet/\")\n",
    "fact_sales_df.count()\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{database_name}.{table_name_partitioned}\")\n",
    "os.system(f\"rm -rf {partitioned_table_path}\")\n",
    "\n",
    "fact_sales_df.sortWithinPartitions(\"ctrl_load_date\").writeTo(f\"{catalog_name}.{database_name}.{table_name_partitioned}\").partitionedBy(F.days(\"ctrl_load_date\")).createOrReplace()\n",
    "print(\"Table created.\")\n",
    "display_metadata(demo_tbl_df=demo_iceberg_df, table_path=partitioned_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM {catalog_name}.{database_name}.{table_name_partitioned} LIMIT 10\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "demo_icerberg_rewrite_table = spark.table(f\"{catalog_name}.{database_name}.{table_name}\")\n",
    "SparkActions\n",
    "    .get()\n",
    "    .rewriteDataFiles(table)\n",
    "    .option(\"target-file-size-bytes\", Long.toString(500 * 1024 * 1024))\n",
    "    .execute();\n",
    "\n",
    "spark.table(f\"{catalog_name}.{database_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
